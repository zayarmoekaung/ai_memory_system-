import tiktoken\nfrom typing import List, Dict, Any\nfrom ..config.settings import settings # Corrected import path\nfrom nltk.tokenize import sent_tokenize\n\n# Ensure NLTK data is available (run this once if not already downloaded)\n# try:\n#     sent_tokenize(\"test\")\n# except LookupError:\n#     import nltk\n#     nltk.download(\'punkt\')\n\nclass ChunkOptimizer:\n    def __init__(self, tokenizer_model_name: str = \"gpt-4\"):\n        \"\"\"\n        Initializes the ChunkOptimizer with a tokenizer.\n        Args:\n            tokenizer_model_name (str): The name of the model to get the tokenizer for (e.g., \'gpt-4\').\n                                        Defaults to \'gpt-4\' but can be overridden by EmbeddingManager\'s tokenizer.\n        \"\"\"\n        self.tokenizer = tiktoken.encoding_for_model(tokenizer_model_name)\n\n    def set_tokenizer(self, tokenizer: Any):\n        \"\"\"\n        Sets the tokenizer to be used, typically from an EmbeddingManager.\n        Args:\n            tokenizer: The tokenizer instance from the embedding model.\n        \"\"\"\n        self.tokenizer = tokenizer\n\n    def chunk_text(self, text: str) -> List[str]:\n        \"\"\"\n        Segments raw text into smaller, meaningful chunks based on sentences.\n\n        Args:\n            text (str): The raw input text.\n\n        Returns:\n            List[str]: A list of text chunks.\n        \"\"\"\
        sentences = sent_tokenize(text)\n        chunks = []\n        current_chunk = []\n        for sentence in sentences:\n            current_chunk.append(sentence)\n            if len(current_chunk) >= settings.CHUNK_SIZE_SENTENCES:\n                chunks.append(\" \".join(current_chunk))\n                # Implement overlap by keeping the last few sentences\n                current_chunk = current_chunk[-settings.CHUNK_OVERLAP_SENTENCES:]\n        if current_chunk:\n            chunks.append(\" \".join(current_chunk))\n        return chunks\n\n    def count_tokens(self, text: str) -> int:\n        \"\"\"\n        Uses the tokenizer to accurately count tokens for a given text.\n\n        Args:\n            text (str): The input text.\n\n        Returns:\n            int: The number of tokens in the text.\n        \"\"\"\
        return len(self.tokenizer.encode(text))\n\n    def optimize_chunks_for_context(self, chunks: List[Dict[str, Any]], query_tokens: int) -> List[Dict[str, Any]]:\n        \"\"\"\n        Selects and potentially truncates chunks to fit within the maximum context window.\n        Assumes chunks come with a \'score\' key (e.g., combined similarity, recency, importance).\n        Includes a conceptual placeholder for rudimentary synthesis/summarization.\n\n        Args:\n            chunks (List[Dict[str, Any]]): A list of retrieved memory chunks,\n                                          each expected to have \'content\' and \'score\'.\n            query_tokens (int): The number of tokens already consumed by the query or prompt itself.\n\n        Returns:\n            List[Dict[str, Any]]: An optimized list of chunks that fit within the token limit,\n                                  potentially truncated or synthesized.\n        \"\"\"\
        if not chunks: # Handle empty chunks list\n            return []\n\n        # Sort chunks by score in descending order\n        sorted_chunks = sorted(chunks, key=lambda x: x.get(\'score\', 0), reverse=True)\n\n        optimized_selected_chunks = []\n        current_tokens = query_tokens\n\n        for chunk in sorted_chunks:\n            chunk_content = chunk.get(\'content\', \'\')\n            chunk_tokens = self.count_tokens(chunk_content)\n\n            if current_tokens + chunk_tokens <= settings.MAX_CONTEXT_TOKENS:\n                optimized_selected_chunks.append(chunk)\n                current_tokens += chunk_tokens\n            else:\n                # Conceptual Placeholder: Dynamic Synthesis/Summarization\n                # If many relevant chunks are competing for space, or a long chunk needs to be fit,\n                # a small LLM or heuristic could synthesize a shorter version.\n                # For now, we continue with truncation.\n                # if _can_be_synthesized(chunk_content, remaining_tokens):\n                #     synthesized_content = _perform_synthesis(chunk_content, remaining_tokens)\n                #     ... add synthesized_content ...\n\n                # Attempt truncation if adding the full chunk exceeds limit\n                remaining_tokens = settings.MAX_CONTEXT_TOKENS - current_tokens\n                if remaining_tokens > 0:\n                    truncated_content = self._truncate_text_by_tokens(chunk_content, remaining_tokens)\n                    if truncated_content: # Only add if truncation resulted in some content\n                        truncated_chunk = chunk.copy()\n                        truncated_chunk[\'content\'] = truncated_content\n                        optimized_selected_chunks.append(truncated_chunk)\n                        current_tokens += self.count_tokens(truncated_content)\n                break # No more room for further chunks\n\n        return optimized_selected_chunks\n\n    def _truncate_text_by_tokens(self, text: str, max_tokens: int) -> str:\n        \"\"\"\n        Truncates text to a maximum number of tokens, trying to respect sentence boundaries.\n        \"\"\"\
        if max_tokens <= 0: # Ensure max_tokens is positive\n            return \"\"\n\n        tokens = self.tokenizer.encode(text)\n        if len(tokens) <= max_tokens:\n            return text\n\n        # Decode tokens up to max_tokens\n        truncated_tokens = tokens[:max_tokens]\n        truncated_text = self.tokenizer.decode(truncated_tokens)\n\n        # Attempt to end at a natural sentence boundary if possible\n        sentences = sent_tokenize(truncated_text)\n        if sentences:\n            # Try to include as many full sentences as possible without exceeding max_tokens\n            final_text_parts = []\n            current_token_count = 0\n            for sentence in sentences:\n                sentence_tokens = self.count_tokens(sentence)\n                if current_token_count + sentence_tokens <= max_tokens:\n                    final_text_parts.append(sentence)\n                    current_token_count += sentence_tokens\n                else:\n                    break\n            if final_text_parts:\n                return \" \".join(final_text_parts)\n        \n        return truncated_text.rsplit(\' \', 1)[0] + \"...\" if \' \' in truncated_text else truncated_text # Fallback to word split if no sentence boundary\n\n# Example usage (for testing purposes)\nif __name__ == \"__main__\":\n    # Make sure NLTK punkt tokenizer data is downloaded for sent_tokenize\n    # import nltk\n    # nltk.download(\'punkt\')\n\n    # Dummy chunks for testing optimize_chunks_for_context\n    test_chunks = [\n        {\'id\': \'c1\', \'content\': \'This is a very important chunk of information. It talks about many things. The first thing is crucial.\', \'score\': 0.9},\n        {\'id\': \'c2\', \'content\': \'This is another chunk, less important than the first. It provides some background details.\', \'score\': 0.7},\n        {\'id\': \'c3\', \'content\': \'A third piece of data, quite long and less relevant. It contains a lot of filler words and extra context that might not be strictly necessary.\', \'score\': 0.5},\n        {\'id\': \'c4\', \'content\': \'A final, moderately important chunk.\', \'score\': 0.8},\n    ]\n\n    # Initialize ChunkOptimizer (using default gpt-4 tokenizer for now)\n    chunk_optimizer = ChunkOptimizer()\n\n    # Test chunk_text\n    long_text = \"This is the first sentence. This is the second sentence. This is the third sentence. And here is the fourth one. Finally, the fifth sentence. A sixth sentence to test overlap.\"\n    print(f\"\\nOriginal text: {long_text}\")\n    text_chunks = chunk_optimizer.chunk_text(long_text)\n    print(f\"Chunked text ({len(text_chunks)} chunks):\")\n    for i, chunk in enumerate(text_chunks):\n        print(f\"  Chunk {i+1} ({chunk_optimizer.count_tokens(chunk)} tokens): {chunk}\")\n\n    # Test count_tokens\n    sample_text = \"Hello, world! How are you doing today?\"\n    tokens_count = chunk_optimizer.count_tokens(sample_text)\n    print(f\"\\n\'{sample_text}\' has {tokens_count} tokens.\")\n\n    # Test optimize_chunks_for_context with a mock MAX_CONTEXT_TOKENS (e.g., 50 tokens)\n    # Simulate settings.MAX_CONTEXT_TOKENS and query_tokens\n    original_max_tokens = settings.MAX_CONTEXT_TOKENS\n    settings.MAX_CONTEXT_TOKENS = 50 # Temporarily set a small limit for testing\n    query_tokens_mock = 10\n\n    print(f\"\\nOptimizing chunks for context (MAX_CONTEXT_TOKENS={settings.MAX_CONTEXT_TOKENS}, query_tokens={query_tokens_mock})...\")\n    optimized_chunks = chunk_optimizer.optimize_chunks_for_context(test_chunks, query_tokens_mock)\n\n    total_optimized_tokens = query_tokens_mock + sum(self.count_tokens(c[\'content\']) for c in optimized_chunks)\n    print(f\"Total tokens after optimization: {total_optimized_tokens}\")\n\n    for i, chunk in enumerate(optimized_chunks):\n        print(f\"  Optimized Chunk {i+1} (Score: {chunk.get(\'score\', \'N/A\')}, Tokens: {self.count_tokens(chunk[\'content\'])}): {chunk[\'content\']}\")\n    \n    # Restore original setting\n    settings.MAX_CONTEXT_TOKENS = original_max_tokens\n