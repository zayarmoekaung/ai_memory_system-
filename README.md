# AI Agent Memory System\n\nThis project implements a sophisticated memory system for AI agents, designed to efficiently store, retrieve, and optimize contextual information. It leverages vector embeddings for semantic search, weighted retrieval based on factors like recency and importance, and dynamic chunk selection to manage token usage within large language model (LLM) context windows.\n\n## Features\n\n*   **Vectorizing Memories:** Converts raw text memories into high-dimensional vector embeddings using pre-trained `sentence-transformers` models.\n*   **Weighted Retrieval:** Retrieves relevant memory chunks based on a combination of vector similarity, recency, and importance scores.\n*   **Dynamic Chunk Selection:** Segments raw text into manageable chunks, accurately counts tokens, and intelligently selects/truncates chunks to fit within specified token limits.\n*   **Persistent Storage:** Utilizes ChromaDB for efficient and persistent storage of memory chunks and their associated metadata.\n*   **Configurable:** All key parameters (embedding model, token limits, weighting coefficients) are centralized in a `settings.py` file.\n*   **Optional API:** Provides a FastAPI interface for external services or agents to interact with the memory system programmatically.\n\n## Human-like Memory Prototype (Work in Progress)\n\nThis section outlines the conceptual enhancements and initial structural changes aimed at making the AI Memory System emulate human memory more closely. This involves a richer understanding of memory elements and their interconnections.\n\n**Key Prototype Components & Concepts:**\n\n*   **Sensory Input Buffer (Working Memory):** A new, temporary, limited-capacity in-memory store for immediate context and ongoing thoughts, acting as the fastest recall layer.\n*   **Memory Consolidation:** A conceptual background process for reviewing working memories, summarizing them, assigning enhanced metadata, and integrating them into long-term storage.\n*   **Associative Network:** A lightweight graph structure for managing explicit links and relationships between memory chunks and extracted entities, enabling "spreading activation" during retrieval.\n*   **Enhanced Long-Term Memory Metadata:** ChromaDB is extended to store richer metadata for each chunk, including `associated_entities`, `emotional_valence`, `vividness_score`, `context_tags`, and `event_sequence_id`.\n*   **Adaptive Ingestion Flow:** Introduces intelligent selection of memory candidates from working memory based on information density, topic shifts, and actionable items, along with pre-processing for basic NLP extraction of metadata.\n*   **Dynamic & Associative Retrieval Flow:** Prioritizes working memory, incorporates associative spreading, and uses a refined weighted scoring that includes emotional salience, associative strength, and vividness, alongside similarity, recency, and importance.\n*   **Dynamic Synthesis & Focal Attention:** The `ChunkOptimizer` is conceptually enhanced to not just truncate but also to potentially synthesize or summarize relevant chunks for more coherent recollections.\n\n## Project Structure\n\n```\nai_memory_system/\n├── src/\n│   ├── core/\n│   │   ├── memory_store.py         # Manages memory chunks, metadata, and interaction with ChromaDB\n│   │   ├── embedding_manager.py    # Handles embedding model loading and vectorization\n│   │   ├── retrieval_manager.py    # Implements weighted retrieval logic\n│   │   ├── chunk_optimizer.py      # Handles dynamic chunk selection, token counting, and truncation\n│   │   ├── working_memory.py       # Sensory Input Buffer (new)\n│   │   ├── memory_consolidation.py # Background process for memory consolidation (new)\n│   │   ├── associative_network.py  # Manages explicit links between memories/entities (new)\n│   │   └── __init__.py\n│   ├── api/                      # REST API interface using FastAPI\n│   │   ├── main.py\n│   │   └── __init__.py\n│   └── __init__.py\n├── tests/                        # Unit and integration tests\n├── config/\n│   ├── settings.py               # Centralized configuration parameters\n├── data/                         # Directory for ChromaDB persistence\n├── examples/                     # Demonstration and interactive usage scripts\n│   ├── basic_usage.py\n│   ├── interactive_test.py\n├── requirements.txt              # Project dependencies\n├── implementation_notes.md       # Detailed implementation plan and progress\n├── Shion.md                      # ShionAide's contributions and agent instructions\n└── README.md                     # Project overview and instructions\n```\n\n## Technologies Used\n\n*   **Python 3.9+:** Core programming language.\n*   **ChromaDB:** Vector database for memory storage and similarity search (now with two collections: main memory and associative links).\n*   **`sentence-transformers`:** For generating robust text embeddings.\n*   **`tiktoken`:** For accurate token counting, consistent with OpenAI models.\n*   **`pydantic` / `pydantic-settings`:** For robust data validation and configuration management.\n*   **`numpy`:** For efficient numerical operations.\n*   **`FastAPI` & `Uvicorn`:** (Optional) For building a high-performance web API.\n*   **`python-dotenv`:** For loading environment variables.\n*   **`nltk`:** For sentence tokenization (requires `punkt` data).\n*   **`networkx`:** (New, for prototype) For the conceptual `AssociativeNetwork` graph structure.\n\n## Setup Instructions\n\n1.  **Clone the Repository:**\n    ```bash\n    git clone git@github.com:zayarmoekaung/ai_memory_system-.git\n    cd ai_memory_system\n    ```\n\n2.  **Create and Activate a Virtual Environment:**\n    ```bash\n    python -m venv venv\n    source venv/bin/activate  # On Windows, use `.\venv\Scripts\activate`\n    ```\n\n3.  **Install Dependencies:**\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n4.  **Download NLTK Data (required for ChunkOptimizer):**\n    The `ChunkOptimizer` uses `nltk` for sentence tokenization. You will need to download the `punkt` tokenizer data.\n    You can do this by running Python:\n    ```python\n    import nltk\n    nltk.download(\'punkt\')\n    ```\n\n5.  **Create an `.env` file (optional but recommended):**\n    While default values are provided in `config/settings.py`, you can override them by creating a `.env` file in the `ai_memory_system/` directory. For example:\n    ```env\n    EMBEDDING_MODEL_NAME=\"all-MiniLM-L6-v2\"\n    MAX_CONTEXT_TOKENS=8000\n    WORKING_MEMORY_CAPACITY=15\n    # ... other settings\n    ```\n\n## Usage Examples\n\n### Basic Usage\n\nRun the `basic_usage.py` script to see a simple demonstration of memory ingestion and retrieval:\n\n```bash\npython examples/basic_usage.py\n```\n\n### Interactive Test\n\nRun the `interactive_test.py` script to interact with the memory system via a command-line interface:\n\n```bash\npython examples/interactive_test.py\n```\n\n### Running the API (Optional)\n\nIf you want to expose the memory system as a REST API, you can run the FastAPI application:\n\n```bash\n# Ensure you are in the ai_memory_system/ directory\nuvicorn src.api.main:app --reload --host 0.0.0.0 --port 8000\n```\n\nThis will start the API server, typically accessible at `http://localhost:8000/docs` for interactive documentation.\n\n## Testing\n\n(Details on how to run tests will be added here as tests are implemented.)\n